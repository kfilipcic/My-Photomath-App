{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Photomath\" application\n",
    "\n",
    "Application for reading an input image containing a handwritten math expression, parsing the output and calculating the result.\n",
    "\n",
    "The handwritten character detector is implemented using `OpenCV` library. It first removes noises from image using `cv3.GaussianBlur` and morphological transformations (`cv2.threshold`, `cv2.dilate`), then finds contours using `cv2.findContours` function. It first finds only external contours on the input image, then for each cropped image, it only takes the biggest found contour and deletes (paints the area black) all the smaller ones. This also counts as removing noise. After testing various input images, I find that this works good (without complicating it further).\n",
    "\n",
    "Regarding the handwritten character classifier, it uses a Convolutional Neural Network with ReLu activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading input and removing noise from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading input image\n",
    "img = cv2.imread('slika_test.jpg', 0)\n",
    "img_original = img\n",
    "\n",
    "h5_filename = 'cnn_math_exp_detection_9720_sgd.h5'\n",
    "\n",
    "# Removing noise / preprocessing\n",
    "img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "img = cv2.bitwise_not(img)\n",
    "\n",
    "# Using Morphological transformations for removing noise in the input image\n",
    "\n",
    "# Kernel for erosion and dilation\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "\n",
    "# cv2.morphologyEx function sometimes works perfectly,\n",
    "# but sometimes ruins the entire input\n",
    "#img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "img = cv2.threshold(img, 160, 255, cv2.THRESH_BINARY)[1]\n",
    "# Dilation mostly helps\n",
    "img = cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "cv2.imwrite('input_image_processed.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating characters (contours) and removing noise from each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_contours(contours):\n",
    "    # Sort contours from left to right\n",
    "    boundingBoxes = [cv2.boundingRect(c) for c in contours]\n",
    "    (cnts, boundingBoxes) = zip(*sorted(zip(contours, boundingBoxes),\n",
    "        key=lambda b:b[1][0]))\n",
    "    return (cnts, boundingBoxes)\n",
    "\n",
    "# Find contours\n",
    "contours, hierarchy = cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours, boundingBoxes = sort_contours(contours)\n",
    "\n",
    "if os.path.isdir('test_image_unprepared'):\n",
    "    shutil.rmtree('./test_image_unprepared')\n",
    "os.mkdir('test_image_unprepared')\n",
    "\n",
    "img_cnt= 0\n",
    "for contour in contours:\n",
    "    x,y,w,h = cv2.boundingRect(contour)\n",
    "    # Take into consideration only contours bigger than 5x5,\n",
    "    # to ignore noises\n",
    "    if w>1 and h>1:\n",
    "        # Save individual images\n",
    "\n",
    "        if img_cnt >= 0:\n",
    "            # Save unaltered cropped images\n",
    "            cv2.imwrite('cropped_characters/' + str(img_cnt) + \".jpg\", img_original[y:y+h,x:x+w])\n",
    "            # Save modified cropped images for\n",
    "            # further processing\n",
    "            img_cropped = img[y:y+h,x:x+w]\n",
    "            \n",
    "            # If there are more than one contours found on one image\n",
    "            # leave out only the biggest one and remove all smaller ones\n",
    "            contours_cropped, hierarchy_cropped = cv2.findContours(img_cropped,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Index of current biggest contour\n",
    "            biggest_contour_idx = 0\n",
    "            \n",
    "            # Find contour with biggest surface\n",
    "            for cnt_crp_idx in range(len(contours_cropped)):\n",
    "                xc,yc,wc,hc = cv2.boundingRect(contours_cropped[cnt_crp_idx])\n",
    "                xcb,ycb,wcb,hcb = cv2.boundingRect(contours_cropped[biggest_contour_idx])\n",
    "                if wc*hc > wcb*hcb:\n",
    "                    biggest_contour_idx = cnt_crp_idx\n",
    "            \n",
    "            # Fill all smaller contour with solid black color\n",
    "            # In other words: remove all smaller contours\n",
    "            for cnt_crp_idx in range(len(contours_cropped)):\n",
    "                if cnt_crp_idx != biggest_contour_idx:\n",
    "                    xc,yc,wc,hc = cv2.boundingRect(contours_cropped[cnt_crp_idx])\n",
    "                    img_shape = img_cropped[yc:yc+hc,xc:xc+wc].shape\n",
    "                    img_cropped[yc:yc+hc,xc:xc+wc] = np.zeros(img_shape)\n",
    "                    \n",
    "            # Save only the biggest contour to image\n",
    "            xcb,ycb,wcb,hcb = cv2.boundingRect(contours_cropped[biggest_contour_idx])\n",
    "            \n",
    "            cv2.imwrite('test_image_unprepared/' + str(img_cnt) + \".jpg\", img_cropped[ycb:ycb+hcb, xcb:xcb+wcb])\n",
    "            \n",
    "            #cv2.imshow('img', img[y:y+h,x:x+w])\n",
    "            #cv2.waitKey(0)\n",
    "            #cv2.destroyAllWindows()\n",
    "        img_cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of each number / math symbol from input\n",
    "\n",
    "In this part noise is removed for each character and processed in order to be in the same format as the training data (MNIST 20x20 format).\n",
    "\n",
    "The `resizeAndPad` function is used for resizing (scaling down) the input image while preserving aspect ratio. \n",
    "\n",
    "Processed characters are then saved to a newly created folder in separate images (for creating test generator which will be used to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-5-6b9d8907e198>:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if len(img.shape) is 3 and not isinstance(padColor, (list, tuple, np.ndarray)): # color image but only one color provided\n"
     ]
    }
   ],
   "source": [
    "# Image preprocessing\n",
    "def resizeAndPad(img, size, padColor=0):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    sh, sw = size\n",
    "\n",
    "    # interpolation method\n",
    "    if h > sh or w > sw: # shrinking image\n",
    "        interp = cv2.INTER_AREA\n",
    "    else: # stretching image\n",
    "        interp = cv2.INTER_CUBIC\n",
    "\n",
    "    # aspect ratio of image\n",
    "    aspect = w/h  # if on Python 2, you might need to cast as a float: float(w)/h\n",
    "\n",
    "    # compute scaling and pad sizing\n",
    "    if aspect > 1: # horizontal image\n",
    "        new_w = sw\n",
    "        new_h = np.round(new_w/aspect).astype(int)\n",
    "        pad_vert = (sh-new_h)/2\n",
    "        pad_top, pad_bot = np.floor(pad_vert).astype(int), np.ceil(pad_vert).astype(int)\n",
    "        pad_left, pad_right = 0, 0\n",
    "    elif aspect < 1: # vertical image\n",
    "        new_h = sh\n",
    "        new_w = np.round(new_h*aspect).astype(int)\n",
    "        pad_horz = (sw-new_w)/2\n",
    "        pad_left, pad_right = np.floor(pad_horz).astype(int), np.ceil(pad_horz).astype(int)\n",
    "        pad_top, pad_bot = 0, 0\n",
    "    else: # square image\n",
    "        new_h, new_w = sh, sw\n",
    "        pad_left, pad_right, pad_top, pad_bot = 0, 0, 0, 0\n",
    "\n",
    "    # set pad color\n",
    "    if len(img.shape) is 3 and not isinstance(padColor, (list, tuple, np.ndarray)): # color image but only one color provided\n",
    "        padColor = [padColor]*3\n",
    "\n",
    "    # scale and pad\n",
    "    scaled_img = cv2.resize(img, (new_w, new_h), interpolation=interp)\n",
    "    scaled_img = cv2.copyMakeBorder(scaled_img, pad_top, pad_bot, pad_left, pad_right, borderType=cv2.BORDER_CONSTANT, value=padColor)\n",
    "\n",
    "    return scaled_img\n",
    "\n",
    "# Input images need to match\n",
    "# match MNIST dataset digits format (28x28)\n",
    "\n",
    "# Create folder which will contain\n",
    "# processed images ready for predicting\n",
    "if os.path.isdir('test_images_prepared'):\n",
    "    shutil.rmtree('test_images_prepared')\n",
    "os.mkdir('test_images_prepared')\n",
    "os.mkdir('test_images_prepared/all_classes')\n",
    "\n",
    "for image_path in os.listdir('test_image_unprepared'):\n",
    "    if '.jpg' not in image_path:\n",
    "        continue\n",
    "    # Read image in grayscale\n",
    "    image = cv2.imread('test_image_unprepared/' + image_path, 0)\n",
    "\n",
    "    # Invert black and white colors\n",
    "    #image = cv2.bitwise_not(image)\n",
    "    #cv2.imshow('img', image)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    #image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "    #cv2.imshow('img', image)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    # Make image binary\n",
    "    image = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY)[1]\n",
    "    # Resize to 20x20 preserving aspect ratio\n",
    "    image = resizeAndPad(image, (20, 20))\n",
    "    # Pad the image so it ends up being 28x28\n",
    "    # (just like MNIST dataset images are)\n",
    "    image = cv2.copyMakeBorder(image, 4, 4, 4, 4, borderType=cv2.BORDER_CONSTANT)\n",
    "    # Save the modified image\n",
    "    #cv2.imshow('img', image)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    image_path_splitted = image_path.split('.')\n",
    "    \n",
    "    # Saving processed cropped characters ready to\n",
    "    # be read by the generator\n",
    "    cv2.imwrite('test_images_prepared/all_classes/' + image_path_splitted[0] + '_mnist' + '.' + image_path_splitted[1], image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The character classifier (CNN model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing a character classifier\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating generators\n",
    "\n",
    "Generators read folders containing data for each class and then split the dataset into training and validation data (in this case we used 80% of data for training, 20% for validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 182162 images belonging to 16 classes.\n",
      "Found 45534 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = '.'\n",
    "\n",
    "train_datagen = ImageDataGenerator(#rescale=1./255,\n",
    "    data_format='channels_first',\n",
    "    validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "   train_dir,\n",
    "   target_size=(28, 28),\n",
    "   color_mode='grayscale',\n",
    "   batch_size=20,\n",
    "   shuffle=True,\n",
    "   classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '*', '(', ')', 'div'],\n",
    "   class_mode='categorical',\n",
    "   subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir, # Same directory as training data\n",
    "    target_size=(28, 28),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', '*', '(', ')', 'div'],\n",
    "    class_mode='categorical',\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import keras\n",
    "keras.backend.set_image_data_format('channels_first')\n",
    "\n",
    "### MODEL TRAINING BLOCK START\n",
    "# Three steps to create a CNN\n",
    "# 1. Convolution\n",
    "# 2. Activation\n",
    "# 3. Pooling\n",
    "# Repeat Steps 1,2,3 for adding more hidden layers\n",
    "nb_filters_1 = 64\n",
    "nb_conv_init = 5\n",
    "# 4. After that make a fully connected network\n",
    "# This fully connected network gives ability to the CNN\n",
    "# to classify the samples\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(nb_filters_1, (nb_conv_init, nb_conv_init), input_shape=(1, 28, 28)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(nb_filters_1, (nb_conv_init, nb_conv_init)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(nb_filters_1,(nb_conv_init, nb_conv_init)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(nb_filters_1, (nb_conv_init, nb_conv_init)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16))\n",
    "\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a optimizer, defining model parameters and fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 1.1052 - accuracy: 0.6910 - val_loss: 1.8981 - val_accuracy: 0.4190\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.6138 - accuracy: 0.8290 - val_loss: 1.0845 - val_accuracy: 0.7760\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.4446 - accuracy: 0.8795 - val_loss: 0.5582 - val_accuracy: 0.8830\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.3545 - accuracy: 0.8980 - val_loss: 0.4644 - val_accuracy: 0.8685\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.3180 - accuracy: 0.9025 - val_loss: 0.2694 - val_accuracy: 0.9230\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2560 - accuracy: 0.9235 - val_loss: 0.4367 - val_accuracy: 0.8600\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2368 - accuracy: 0.9330 - val_loss: 0.2048 - val_accuracy: 0.9450\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.2118 - accuracy: 0.9355 - val_loss: 0.2136 - val_accuracy: 0.9375\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2031 - accuracy: 0.9380 - val_loss: 0.2279 - val_accuracy: 0.9335\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1903 - accuracy: 0.9470 - val_loss: 0.2192 - val_accuracy: 0.9355\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.2006 - accuracy: 0.9400 - val_loss: 0.1915 - val_accuracy: 0.9475\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1585 - accuracy: 0.9485 - val_loss: 0.1968 - val_accuracy: 0.9415\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1797 - accuracy: 0.9445 - val_loss: 0.1286 - val_accuracy: 0.9620\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1789 - accuracy: 0.9550 - val_loss: 0.1332 - val_accuracy: 0.9660\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1615 - accuracy: 0.9540 - val_loss: 0.2558 - val_accuracy: 0.9215\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.1343 - accuracy: 0.9610 - val_loss: 0.1864 - val_accuracy: 0.9475\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1414 - accuracy: 0.9585 - val_loss: 0.1772 - val_accuracy: 0.9425\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1494 - accuracy: 0.9570 - val_loss: 0.1699 - val_accuracy: 0.9540\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1436 - accuracy: 0.9535 - val_loss: 0.1189 - val_accuracy: 0.9635\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1291 - accuracy: 0.9665 - val_loss: 0.1934 - val_accuracy: 0.9345\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1263 - accuracy: 0.9615 - val_loss: 0.1208 - val_accuracy: 0.9675\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1135 - accuracy: 0.9645 - val_loss: 0.1361 - val_accuracy: 0.9585\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1212 - accuracy: 0.9625 - val_loss: 0.1179 - val_accuracy: 0.9640\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1138 - accuracy: 0.9660 - val_loss: 0.1133 - val_accuracy: 0.9695\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1258 - accuracy: 0.9595 - val_loss: 0.1600 - val_accuracy: 0.9535\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1437 - accuracy: 0.9580 - val_loss: 0.1317 - val_accuracy: 0.9610\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1235 - accuracy: 0.9655 - val_loss: 0.1262 - val_accuracy: 0.9645\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1408 - accuracy: 0.9535 - val_loss: 0.1067 - val_accuracy: 0.9690\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1107 - accuracy: 0.9625 - val_loss: 0.1050 - val_accuracy: 0.9705\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1279 - accuracy: 0.9660 - val_loss: 0.1272 - val_accuracy: 0.9605\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0960 - accuracy: 0.9690 - val_loss: 0.1023 - val_accuracy: 0.9685\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1020 - accuracy: 0.9680 - val_loss: 0.0852 - val_accuracy: 0.9740\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1259 - accuracy: 0.9645 - val_loss: 0.1601 - val_accuracy: 0.9465\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0914 - accuracy: 0.9730 - val_loss: 0.0906 - val_accuracy: 0.9720\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1091 - accuracy: 0.9690 - val_loss: 0.1201 - val_accuracy: 0.9645\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1104 - accuracy: 0.9642 - val_loss: 0.1105 - val_accuracy: 0.9670\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0989 - accuracy: 0.9705 - val_loss: 0.1200 - val_accuracy: 0.9630\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1102 - accuracy: 0.9705 - val_loss: 0.1436 - val_accuracy: 0.9595\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1057 - accuracy: 0.9715 - val_loss: 0.0942 - val_accuracy: 0.9720\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1061 - accuracy: 0.9675 - val_loss: 0.1025 - val_accuracy: 0.9755\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0872 - accuracy: 0.9750 - val_loss: 0.1251 - val_accuracy: 0.9590\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1040 - accuracy: 0.9690 - val_loss: 0.1153 - val_accuracy: 0.9670\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0801 - accuracy: 0.9790 - val_loss: 0.1014 - val_accuracy: 0.9675\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0860 - accuracy: 0.9740 - val_loss: 0.1066 - val_accuracy: 0.9675\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.1008 - accuracy: 0.9715 - val_loss: 0.0930 - val_accuracy: 0.9700\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1026 - accuracy: 0.9690 - val_loss: 0.0878 - val_accuracy: 0.9775\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1082 - accuracy: 0.9720 - val_loss: 0.0909 - val_accuracy: 0.9770\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1086 - accuracy: 0.9695 - val_loss: 0.0887 - val_accuracy: 0.9750\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0833 - accuracy: 0.9735 - val_loss: 0.1105 - val_accuracy: 0.9625\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0941 - accuracy: 0.9730 - val_loss: 0.0967 - val_accuracy: 0.9730\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0858 - accuracy: 0.9730 - val_loss: 0.0579 - val_accuracy: 0.9830\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0909 - accuracy: 0.9735 - val_loss: 0.0852 - val_accuracy: 0.9775\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1023 - accuracy: 0.9705 - val_loss: 0.0844 - val_accuracy: 0.9775\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1011 - accuracy: 0.9645 - val_loss: 0.0766 - val_accuracy: 0.9775\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0782 - accuracy: 0.9760 - val_loss: 0.0891 - val_accuracy: 0.9740\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0766 - accuracy: 0.9780 - val_loss: 0.0853 - val_accuracy: 0.9795\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.1203 - accuracy: 0.9590 - val_loss: 0.1100 - val_accuracy: 0.9680\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0931 - accuracy: 0.9665 - val_loss: 0.0956 - val_accuracy: 0.9730\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0638 - accuracy: 0.9810 - val_loss: 0.0632 - val_accuracy: 0.9810\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0943 - accuracy: 0.9735 - val_loss: 0.0748 - val_accuracy: 0.9805\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0691 - accuracy: 0.9775 - val_loss: 0.0658 - val_accuracy: 0.9790\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0871 - accuracy: 0.9725 - val_loss: 0.0820 - val_accuracy: 0.9750\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0626 - accuracy: 0.9800 - val_loss: 0.0697 - val_accuracy: 0.9790\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0796 - accuracy: 0.9765 - val_loss: 0.0902 - val_accuracy: 0.9735\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0861 - accuracy: 0.9765 - val_loss: 0.0916 - val_accuracy: 0.9750\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0776 - accuracy: 0.9795 - val_loss: 0.0745 - val_accuracy: 0.9765\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0681 - accuracy: 0.9835 - val_loss: 0.0790 - val_accuracy: 0.9760\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0788 - accuracy: 0.9740 - val_loss: 0.0630 - val_accuracy: 0.9785\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0968 - accuracy: 0.9730 - val_loss: 0.1119 - val_accuracy: 0.9635\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0883 - accuracy: 0.9720 - val_loss: 0.0745 - val_accuracy: 0.9775\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0949 - accuracy: 0.9710 - val_loss: 0.0844 - val_accuracy: 0.9775\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0800 - accuracy: 0.9755 - val_loss: 0.0703 - val_accuracy: 0.9760\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0740 - accuracy: 0.9785 - val_loss: 0.0915 - val_accuracy: 0.9735\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0808 - accuracy: 0.9745 - val_loss: 0.0776 - val_accuracy: 0.9810\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0918 - accuracy: 0.9715 - val_loss: 0.1063 - val_accuracy: 0.9685\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0976 - accuracy: 0.9705 - val_loss: 0.0606 - val_accuracy: 0.9785\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0741 - accuracy: 0.9760 - val_loss: 0.0639 - val_accuracy: 0.9795\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0850 - accuracy: 0.9750 - val_loss: 0.0588 - val_accuracy: 0.9830\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0731 - accuracy: 0.9780 - val_loss: 0.0545 - val_accuracy: 0.9825\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.1009 - accuracy: 0.9720 - val_loss: 0.0620 - val_accuracy: 0.9825\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0666 - accuracy: 0.9790 - val_loss: 0.0712 - val_accuracy: 0.9780\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0686 - accuracy: 0.9790 - val_loss: 0.0743 - val_accuracy: 0.9760\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0852 - accuracy: 0.9735 - val_loss: 0.0595 - val_accuracy: 0.9825\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0677 - accuracy: 0.9780 - val_loss: 0.0652 - val_accuracy: 0.9815\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0641 - accuracy: 0.9810 - val_loss: 0.0738 - val_accuracy: 0.9790\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0781 - accuracy: 0.9725 - val_loss: 0.0638 - val_accuracy: 0.9800\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0746 - accuracy: 0.9730 - val_loss: 0.0871 - val_accuracy: 0.9765\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.0654 - val_accuracy: 0.9820\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0615 - accuracy: 0.9795 - val_loss: 0.0544 - val_accuracy: 0.9850\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0610 - accuracy: 0.9780 - val_loss: 0.0562 - val_accuracy: 0.9820\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0731 - accuracy: 0.9720 - val_loss: 0.0749 - val_accuracy: 0.9755\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0737 - accuracy: 0.9800 - val_loss: 0.0609 - val_accuracy: 0.9825\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0576 - accuracy: 0.9845 - val_loss: 0.0621 - val_accuracy: 0.9795\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0616 - accuracy: 0.9820 - val_loss: 0.0692 - val_accuracy: 0.9770\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0691 - accuracy: 0.9765 - val_loss: 0.0829 - val_accuracy: 0.9775\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0767 - accuracy: 0.9760 - val_loss: 0.0531 - val_accuracy: 0.9835\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0845 - accuracy: 0.9750 - val_loss: 0.0628 - val_accuracy: 0.9805\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0654 - accuracy: 0.9820 - val_loss: 0.0628 - val_accuracy: 0.9795\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s 11ms/step - loss: 0.0781 - accuracy: 0.9730 - val_loss: 0.0577 - val_accuracy: 0.9805\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0796 - accuracy: 0.9795 - val_loss: 0.0545 - val_accuracy: 0.9835\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "ada = keras.optimizers.Adadelta(learning_rate=1, rho=0.95)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "opt = keras.optimizers.SGD(lr_schedule)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_steps=100,\n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a previously saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously trained model\n",
    "#from keras.models import load_model\n",
    "#model = load_model(h5_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing cropped images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['all_classes/0_mnist.jpg',\n",
       " 'all_classes/1_mnist.jpg',\n",
       " 'all_classes/2_mnist.jpg',\n",
       " 'all_classes/3_mnist.jpg',\n",
       " 'all_classes/4_mnist.jpg',\n",
       " 'all_classes/5_mnist.jpg',\n",
       " 'all_classes/6_mnist.jpg',\n",
       " 'all_classes/7_mnist.jpg',\n",
       " 'all_classes/8_mnist.jpg',\n",
       " 'all_classes/9_mnist.jpg',\n",
       " 'all_classes/10_mnist.jpg',\n",
       " 'all_classes/11_mnist.jpg',\n",
       " 'all_classes/12_mnist.jpg',\n",
       " 'all_classes/13_mnist.jpg',\n",
       " 'all_classes/14_mnist.jpg',\n",
       " 'all_classes/15_mnist.jpg']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create generator from cropped processed images\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    train_dir + '/test_images_prepared',\n",
    "    target_size=(28, 28),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    class_mode=None,)\n",
    "test_generator.reset()\n",
    "\n",
    "import re\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return s\n",
    "\n",
    "def alphanum_key(s):\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "\n",
    "# Sorting images in generator numerically\n",
    "# because contours were sorted and the input\n",
    "# cropped character images were numbered sequentially\n",
    "# (without this images are sorted alphabetically,\n",
    "# which is the wrong order)\n",
    "test_generator.filenames.sort(key=alphanum_key)\n",
    "test_generator.filepaths.sort(key=alphanum_key)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get probabilities for each class\n",
    "predictions = model.predict(test_generator)\n",
    "# Get index of classes with highest probabilities\n",
    "predicted_class_indices = np.argmax(predictions,axis=1)\n",
    "# Convert those indices into actual class names\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "# Array containing predictions for each class or\n",
    "# number/symbol\n",
    "output_math_expression = predictions\n",
    "\n",
    "test_generator.filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(h5_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing then calculating math expression given from the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_output_to_string(output_math_expression):\n",
    "    for i in range(len(output_math_expression)):\n",
    "        # NOTE: Class name wasn't '/' in the first place\n",
    "        # because class names have to match folder names\n",
    "        # which are being used for creating generators\n",
    "        # But '/' can't be a valid folder name\n",
    "        if output_math_expression[i] == 'div':\n",
    "            output_math_expression[i] = '/'\n",
    "        str_output = ''\n",
    "    # Create string from list for math expression\n",
    "    # parser/solver\n",
    "    return str_output.join(output_math_expression)\n",
    "\n",
    "math_expression_string = convert_model_output_to_string(output_math_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '7', '+', '3', '*', '(', '1', '9', '-', '5', '*', '3', ')', '-', '1', '4']\n",
      "Result: 25.0\n"
     ]
    }
   ],
   "source": [
    "# Parsing and calculating a given math\n",
    "# expression in a string format\n",
    "\n",
    "# Examples used for testing\n",
    "# All output correct results\n",
    "\"\"\"\n",
    "math_expression_string = '174*36+(58-2)'\n",
    "math_expression_string = '80-(15-(20-8))*9'\n",
    "math_expression_string = '64/(24/(27/9))'\n",
    "math_expression_string = '((32/8)*(40-31))*200'\n",
    "math_expression_string = '90-(14-(61-5)/(48/6))*9'\n",
    "math_expression_string = '(720-220)*((12-7)*(62-54))'\n",
    "math_expression_string = '((160-40)/2)(9(3000/30))'\n",
    "math_expression_string = '(20-(36/(130-126)))*8'\n",
    "math_expression_string = '54-((42/(18-11))*((19+62)/9))'\n",
    "math_expression_string = '(((9-3)*(4+6))/3)*((12-8)*(33/(90/30)))'\n",
    "\"\"\"\n",
    "\n",
    "# Parsing function takes list as input\n",
    "math_expression_string = list(math_expression_string)\n",
    "print(math_expression_string)\n",
    "\n",
    "# Convert numbers to integers\n",
    "for i in range(len(math_expression_string)):\n",
    "    try:\n",
    "       math_expression_string[i] = int(math_expression_string[i])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "op_priority = {'(': 2, '*': 2, '/': 2, '+': 1, '-': 1}\n",
    "\n",
    "def parse_output(idx):\n",
    "    num = ''\n",
    "    num_stack = []\n",
    "    operator_stack = [] \n",
    "    \n",
    "    def calc(num1, num2, op, same_priority=False):\n",
    "        if not same_priority:\n",
    "            operator_stack.pop()\n",
    "        if op == '*':\n",
    "            return num1*num2\n",
    "        elif op == '/':\n",
    "            return num1/num2\n",
    "        elif op == '+':\n",
    "            return num1+num2\n",
    "        elif op == '-':\n",
    "            return num1-num2\n",
    "        return False\n",
    "    \n",
    "    def pop_stack_and_calc(op):\n",
    "        stack_top_num = num_stack[-1]\n",
    "        num_stack.pop()\n",
    "        num_stack[-1] = calc(num_stack[-1], stack_top_num, op)\n",
    "    \n",
    "    # Calculates operations from left to right\n",
    "    # (in case operations of same priority are in a series)\n",
    "    def left_to_right_calc(steps_back):\n",
    "        result = num_stack[steps_back-1]\n",
    "        for i in range(steps_back, 0):\n",
    "            result = calc(result, num_stack[i], operator_stack[i], True)\n",
    "        i = -1\n",
    "        while i >= steps_back:\n",
    "            operator_stack.pop()\n",
    "            num_stack.pop()\n",
    "            i -= 1\n",
    "        num_stack.pop()\n",
    "        num_stack.append(result)\n",
    "\n",
    "    def handle_operation(operation_string, ending_flag=False):\n",
    "        if not operator_stack:\n",
    "            operator_stack.append(operation_string)\n",
    "        elif op_priority[operation_string] > op_priority[operator_stack[-1]] and not ending_flag:\n",
    "            operator_stack.append(operation_string)\n",
    "        else:\n",
    "            steps_back = -1\n",
    "            if len(operator_stack) > 1:\n",
    "                while steps_back > -len(operator_stack)+1 and op_priority[operator_stack[steps_back-1]] == op_priority[operator_stack[steps_back]]:\n",
    "                    steps_back -= 1\n",
    "                left_to_right_calc(steps_back)\n",
    "            else:\n",
    "                pop_stack_and_calc(operator_stack[-1])\n",
    "            if not ending_flag:\n",
    "                operator_stack.append(operation_string)\n",
    "        \n",
    "    i = idx\n",
    "    while i < len(math_expression_string):\n",
    "        if isinstance(math_expression_string[i], int):\n",
    "            num += str(math_expression_string[i])\n",
    "            # If there is a number after closed brackets,\n",
    "            # it means that we multiply the following number\n",
    "            # with the result from inside brackets\n",
    "            if i > 0 and math_expression_string[i-1] == ')':\n",
    "                operator_stack.append('*')\n",
    "        if not isinstance(math_expression_string[i], int) or (isinstance(math_expression_string[i], int) and i == len(math_expression_string)-1):\n",
    "            if i > 0 and math_expression_string[i-1] == ')' and isinstance(math_expression_string[i], int):\n",
    "                operator_stack.append('*')\n",
    "            if len(num) > 0:\n",
    "                num_stack.append(int(num))\n",
    "                num = ''\n",
    "            \n",
    "            # Print state on stacks\n",
    "            #print('num_stack:', num_stack)\n",
    "            #print('operator_stack', operator_stack)\n",
    "                \n",
    "            if math_expression_string[i] == '(':\n",
    "                if i > 0 and (isinstance(math_expression_string[i-1], int) or math_expression_string[i-1] == ')'):\n",
    "                    operator_stack.append('*')\n",
    "                if i+1 < len(math_expression_string):\n",
    "                    return_dict = parse_output(i+1)\n",
    "                    num_stack.append(return_dict['result'])\n",
    "                    i = return_dict['idx']\n",
    "                    if i == len(math_expression_string)-1:\n",
    "                        i -= 1\n",
    "            elif math_expression_string[i] == '*':\n",
    "                handle_operation('*')\n",
    "            elif math_expression_string[i] == '/':\n",
    "                handle_operation('/')\n",
    "            elif math_expression_string[i] == '+':\n",
    "                handle_operation('+')\n",
    "            elif math_expression_string[i] == '-':\n",
    "                handle_operation('-')\n",
    "            elif math_expression_string[i] == ')' or i == len(math_expression_string)-1:\n",
    "                while len(num_stack) > 1:\n",
    "                    handle_operation(operator_stack[-1], True)\n",
    "                if len(num_stack) > 0:\n",
    "                    return {'idx': i, 'result': float(num_stack[-1])}\n",
    "                else:\n",
    "                    return {'idx': i, 'result': 0}\n",
    "        i += 1\n",
    "                \n",
    "    if len(num_stack) > 0:\n",
    "        return {'idx': i, 'result': float(num_stack[-1])}\n",
    "    return 0\n",
    "\n",
    "result = parse_output(0)\n",
    "\n",
    "print(\"Result:\", result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibook",
   "language": "python",
   "name": "aibook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
